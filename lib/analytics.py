import pickle
import itertools
import networkx as nx
import numpy as np
import pandas as pd
from typing import List, Union, Tuple, Callable, Set
import os
from scipy.spatial.distance import pdist
from networkx.drawing.nx_pydot import graphviz_layout
import matplotlib.pyplot as plt
import multiprocessing

from configs.structural_config import PARTICLE_VOLUME
from configs.inheritance_configs import CHAIN_LENGTH
from lib.simulation_handler import Simulation
from lib.file_utils import getFilenames
from lib.graph import (floodFillFromNode, getInheritedChain, getLargestSubnet, getLongestChain,
                       getLargestComponentSize, getChainLength, simplifyDG, recreate_graph_at_time_resolution)
from utils.time_utils import time_alignment

# Start of unwrapped functions from Experiment class for use in multiprocessing
# See experiment class below for more information on useage


def unwrapped_experiment_getAggReductionMitoEvents(arg, **kwarg):
    return Experiment.getAggReductionMitoEvents(*arg,**kwarg)


def unwrapped_experiment_getEventFrequency(arg, **kwarg):
    return Experiment.getEventFrequency(*arg, **kwarg)


def unwrapped_experiment_getInheritedEndAttr(arg, **kwarg):
    return Experiment.getInheritedEndAttr(*arg, **kwarg)


def unwrapped_experiment_getInheritanceOverTime(arg, **kwarg):
    return Experiment.getInheritanceOverTime(*arg, **kwarg)


def unwrapped_experiment_getEndNAggClusters(arg, **kwarg):
    return Experiment.getEndNAggClusters(*arg, **kwarg)


def unwrapped_experiment_getDeltaAggInterSep(arg, **kwarg):
    return Experiment.getDeltaAggInterSep(*arg, **kwarg)


def unwrapped_experiment_getFirstAggFusionEventTime(arg, **kwarg):
    return Experiment.getFirstAggFusionEventTime(*arg, **kwarg)


def unwrapped_experiment_getNSubnetsOverTime(arg, **kwarg):
    return Experiment.getNSubnetsOverTime(*arg, **kwarg)


def unwrapped_experiment_getVolumesOverTime(arg, **kwarg):
    return Experiment.getVolumesOverTime(*arg, **kwarg)


def unwrapped_experiment_getVolumesAtTime(arg, **kwarg):
    return Experiment.getVolumesAtTime(*arg, **kwarg)


def unwrapped_experiment_getNAggsOverTime(arg, **kwarg):
    return Experiment.getNAggsOverTime(*arg, **kwarg)


# End of unwrapped functions from Experiment class for use in multiprocessing


class Experiment:
    experimentSubclass = Simulation

    def __init__(self, folder_path: str,
                 lookup_terms: Union[List[str], str],
                 column_preheader: Tuple[str]=None,
                 id_lookup_len: int = 7):
        """Experiment consists of a directory of pickle files generated by driver.py.
        Each pickle file has a prefix of length id_lookup_len, where the length is 
        a number of strings separated by "_". For example, if a file has the header:
        A_B_C_D_E.pickle, and the lookup length is set to 3, the first three elements
        (A,B,C) will be used as elements to determine the parameters used in running the simulation. 

        Generates dictionary of runIDs matched to their post/pre pickle simulation recordings

        Args:
            folder_path (str): Path to experiment folder
            lookup_terms (Union[List[str], str]): Strings to look up pickle files
            column_preheader (Tuple[str], optional): tuple of strings that are meant to be used as headers in the output
                directory.  Corresponds to the lookup ID for each pickle file.
                When not defined, defaults to ("n_aggs_setting", "diff_setting","ffi_setting", "targFis_setting")
                later on in initalization, where each item corresponds to :
                    n_aggs_setting -> Number of discrete aggregate clusters simulation started with at transition period
                    diff_setting -> Diffusion settings used for simulation
                    ffi_setting -> Fission fusion probability settings used for simulation
                    targFis_setting -> whether targeted/biased fission was set, and with what probability (values 
                        displayed as value from 0-100).
            id_lookup_len (int, optional): Length of runID. Defaults to 7.
        """
        self.folder_path = folder_path
        self.lookup_terms = lookup_terms
        self.id_lookup_len = id_lookup_len

        self.filepath_list = getFilenames(self.folder_path, self.lookup_terms)

        self.unique_runIDs = {}

        if column_preheader is not None:
            self.column_pre_header = column_preheader
        else:
            self.column_pre_header = ("n_aggs_setting", "diff_setting",
                                  "ffi_setting", "targFis_setting")

        for filepath in self.filepath_list:
            filebasename = os.path.basename(filepath)
            fileID = filebasename.split("_")[:self.id_lookup_len]
            fileID = "_".join(fileID)
            case_key = "pre" if "PRE.pickle" in filebasename else "post"
            if fileID not in self.unique_runIDs:
                self.unique_runIDs[fileID] = {"pre": None,
                                              "post": None}
                self.unique_runIDs[fileID][case_key] = filepath
            else:
                self.unique_runIDs[fileID][case_key] = filepath

        # secondary screening which gets rid of entries where either the pre-simulation state
        # or the post simulation state fiels do not exist
        keys2rm = []
        for k, v in self.unique_runIDs.items():
            if v.get('pre') is None:
                keys2rm += [k]
            if v.get('post') is None:
                keys2rm += [k]
        [self.unique_runIDs.pop(k) for k in keys2rm]

    def getExptConditions(self, fileID: str) -> Tuple[int, str, str, str, str]:
        """Helper function to get run experimental conditions
        fileID must be in the following format:
        [DATETIME]_[EXPERIMENT_NAME]_[NUMBER_OF_AGGREGATES]_[DIFFUSIVE_SETTINGS]_[FISSION_FUSION_SETTINGS]_[TARGETED_FISSION_SETTINGS]

        This function takes in the last five elements of the fileID and uses it to determine the experimental conditions used for the run
        Specifically, the conditions recorded are:
        * [NUMBER_OF_AGGREGATES]
        * [DIFFUSIVE_SETTINGS]
        * [FISSION_FUSION_SETTINGS]
        * [TARGETED_FISSION_SETTINGS]

        Args:
            fileID (str): fileID from self.unique_runIDs;
            cannot be file basename generated by os.basename

        Returns:
            Tuple[int, str, str, str, str]: Tuple containing values for (in order):
                nAgg setting
                diffusion coefficient settings
                fission/fusion/idle probability settings
                targeted Fission On/Off
        """
        fileID_descriptor_lst = fileID.split("_")
        n_aggs_setting = int(fileID_descriptor_lst[2][-1])
        diff_setting = fileID_descriptor_lst[3]
        ffi_setting = fileID_descriptor_lst[4]
        targFis_setting = fileID_descriptor_lst[5]

        return (n_aggs_setting, diff_setting, ffi_setting, targFis_setting)

    def viewPrePost(self) -> None:
        """Helper function for visualization of each individual run and its pre and post state.
        """
        for runID, run_simPathDict in self.unique_runIDs.items():
            with open(run_simPathDict['pre'], 'rb') as f:
                pre_case = pickle.load(f)

            with open(run_simPathDict['post'], 'rb') as f:
                post_case = pickle.load(f)

            pre_case.drawSelf()
            pos = [p['pos'] for _, p in pre_case.network.nodes(data=True)]
            c = [p['has_agg'] for _, p in pre_case.network.nodes(data=True)]
            nx.draw(pre_case.network, pos=pos, node_color=c, with_labels=True)
            plt.show()
            pos = [p['pos'] for _, p in pre_case.network.nodes(data=True)]
            c = [p['obj'].has_agg for _,
                 p in pre_case.network.nodes(data=True)]
            nx.draw(pre_case.network, pos=pos, node_color=c, with_labels=True)
            plt.show()

            post_case.drawSelf()
            pos = [p['pos'] for _, p in post_case.network.nodes(data=True)]
            c = [p['has_agg'] for _, p in post_case.network.nodes(data=True)]
            nx.draw(post_case.network, pos=pos, node_color=c, with_labels=True)
            plt.show()
            pos = [p['pos'] for _, p in post_case.network.nodes(data=True)]
            c = [p['obj'].has_agg for _,
                 p in post_case.network.nodes(data=True)]
            nx.draw(post_case.network, pos=pos, node_color=c, with_labels=True)
            plt.show()

    def viewReducedHistory(self) -> None:
        """Plot a reduced history plot for fission/fusion events for each simulation
        Also provides secondary plot of number of aggregates over the course of the simulation.
        """
        for runID, run_simPathDict in self.unique_runIDs.items():
            with open(run_simPathDict['pre'], 'rb') as f:
                pre_case = pickle.load(f)

            with open(run_simPathDict['post'], 'rb') as f:
                post_case = pickle.load(f)

            post_case.drawReducedHistory()
            plt.plot(post_case.nAgg_history)
            plt.show()

    def _getNodeOpHist(self, history_network: nx.DiGraph,
                       query_nodeID: int, MOSubCat: bool = False,
                       time_precision: int = 1) -> Tuple[str, str, int, int, int, int]:
        """Given a selected directed graph to traverse, and a query node ID, determine what 
        operations generated that node and what operations follow that node. Possible options are 
        (Fission, Fusion, None, MultiOp, generation, dissolution). 

            Fission - where one objects breaks up into two or more structures
            Fusion - where multiple objects fuse into one structure
            None - object does not interact constructively with other objects
            MultiOp - object undergoes a series of compound fission/fusion events that are unable to be resolved
            generation - object spontaneously appears
            dissolution - object disappears

        If MOSubCat is set to True, breaks down MultiOp category into 
        arrMO - where a multiop occurs, but the number of parents  = number of siblings generated
        fragMO - where a multiop occurs, but the number of siblings generated is greater than parents
        aggMO - where a multiop occurs, but the number of siblings generated is less than parents


        Args:
            history_network (nx.DiGraph): Directed history graph of structure operations over time
            query_NID (int): node ID 
            MOSubCat (bool, optional): Whether or not to break MultiOps into their subcategories. Defaults to False.
            time_precision (int, optional): Decimal precision to round timestamps to. Defaults to 1.


        Returns:
            Tuple[str, str, int, int, int, int]: (preOp, postOp, nParent, nSibling, nSpouse, nChild)
        """
        all_timestamps = [np.around(ndata.get("timestamp"), decimals=time_precision)
                          for _, ndata in history_network.nodes(data=True)]
        min_timestamp = min(all_timestamps)
        max_timestamp = max(all_timestamps)

        node_timestamp = history_network.nodes.get(
            query_nodeID).get('timestamp')
        parents = set(history_network.predecessors(query_nodeID))
        children = set(history_network.successors(query_nodeID))

        siblings = set([sibling for p in parents for sibling in list(
            history_network.successors(p)) if sibling != query_nodeID])
        spouses = set([spouse for child in children for spouse in list(
            history_network.predecessors(child)) if spouse != query_nodeID])
        nParent = len(parents)
        nChild = len(children)
        nSpouse = len(spouses)
        nSibling = len(siblings)

        preOp = "None"
        if nParent == 1 and nSibling == 0:
            pass
        elif nParent == 1 and nSibling > 0:
            preOp = "Fission"
        elif nParent > 1 and nSibling == 0:
            preOp = "Fusion"
        elif nParent > 1 and nSibling > 0:
            preOp = 'MultiOp'
            if MOSubCat:
                ratio = nParent / (nSibling + 1)
                if ratio == 1:
                    preOp = "arrMO"
                elif ratio < 1:
                    preOp = "fragMO"
                else:
                    preOp = "aggMO"
        elif nParent == 0 and node_timestamp != min_timestamp:
            preOp = "generation"

        postOp = "None"
        if nChild == 1 and nSpouse == 0:
            pass
        elif nChild == 1 and nSpouse > 0:
            postOp = "Fusion"
        elif nChild > 1 and nSpouse == 0:
            postOp = "Fission"
        elif nChild > 1 and nSpouse > 0:
            postOp = "MultiOp"
            if MOSubCat:
                ratio = (nSpouse + 1) / (nChild)
                if ratio == 1:
                    postOp = "arrMO"
                elif ratio < 1:
                    postOp = "fragMO"
                else:
                    postOp = "aggMO"
        elif nChild == 0 and node_timestamp != max_timestamp:
            postOp = "dissolution"

        return (preOp, postOp, nParent, nSibling, nSpouse, nChild)

    def _isolateMitoBallStructureHistory(self, history_network: nx.DiGraph, particleIDs: Set[int]) -> None:
        """Given a set of particleIDs, isolate structures from the history network that contain structures
        that contain the particles listed in particleIDs.  Rather than viewing the whole history map all at once,
        examines only the relevant history of the provided particles in question.  Also includes particles that are 
        associated with the particleIDs provided, but does not provide their history (otherwise that would include the 
        whole history network).

        Provides a simplified history graph, where repetitive "None" events are summarized into no none-events.  This 
        helps with downstream analysis in which we want to examine what events are followed by other events, without 
        dilution by none events.

        Args:
            history_network (nx.DiGraph): history network to peruse.  Note, history network should have particleIDs present;
                make sure that there is only one history_network per simulation 
            particleIDs (Set[int]): Set of Ints representing IDs of particles to examine.
        """

        valid_history_nIDs = []

        for history_nID, history_nData in history_network.nodes(data=True):
            if particleIDs.intersection(history_nData.get("members")):
                valid_history_nIDs += [history_nID]

        history_subset = history_network.subgraph(valid_history_nIDs).copy()

        outputGraph = simplifyDG(history_subset)
        # Plotter for debugging
        # pos = nx.nx_agraph.graphviz_layout(outputGraph,
        #                                    prog='dot',
        #                                    args="-Grankdir=LR")
        # node_color = [outputGraph.nodes[node]['timestamp']
        #               for node in outputGraph.nodes()]
        # cmap = plt.cm.Reds
        # nx.draw_networkx(outputGraph,
        #                  pos=pos,
        #                  node_color=node_color,
        #                  node_shape='o',
        #                  cmap=cmap,
        #                  with_labels=False,
        #                  node_size=50,
        #                  connectionstyle="angle, angleA=-90,angleB=180, rad=0")
        # sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=min(node_color),
        #                                                          vmax=max(node_color)))
        # sm._A = []
        # cbar = plt.colorbar(sm)
        # cbar.set_label("timestamp")
        # plt.show()
        return outputGraph

    def getAggReductionMitoEvents(self, runID: str, pre_pickle_path: str, post_pickle_path: str, cutoff_time: float = 1800) -> pd.DataFrame:
        try:
            with open(post_pickle_path, 'rb') as f:
                post_case = pickle.load(f)
                
            conditions = self.getExptConditions(runID)

            rounded_timestamp2network = {
                np.around(k, decimals=1): v for k, v in post_case.network_history.items()}

            matched_nAgg2Timestamp = []
            for k in rounded_timestamp2network.keys():
                matched_nAgg2Timestamp += [[k,
                                            post_case.nAgg_history[int(k/post_case.time_interval)]]]

            matched_nAgg2Timestamp = np.array(matched_nAgg2Timestamp)
            # Remove any data points before cutoff to reduce computation if desired by user
            if cutoff_time is not None:
                matched_nAgg2Timestamp = matched_nAgg2Timestamp[cutoff_time <
                                                                matched_nAgg2Timestamp[:, 0]]
            # Sort by first column value
            matched_nAgg2Timestamp = matched_nAgg2Timestamp[matched_nAgg2Timestamp[:, 0].argsort(
            )]

            # Get timestamps (keys to dictionary) where there was a decrease in the # of aggregates:
            nAgg_decrease_timeKeys = []
            prior_aggCount = 0
            prior_timestamp = 0
            
            n_reduction_events = 0
            output_dataframe = []
            for timestamp, nAgg in matched_nAgg2Timestamp:
                if nAgg < prior_aggCount:
                    nAgg_decrease_timeKeys += [[prior_timestamp, timestamp,
                                                prior_aggCount, nAgg, nAgg-prior_aggCount]]


                    at_fusion_graph = rounded_timestamp2network.get(timestamp)
                    pre_fusion_graph = rounded_timestamp2network.get(prior_timestamp)
                    rm_nodes = []
                    for nID, nData in at_fusion_graph.nodes(data = True):
                        if not nData['has_agg']:
                            rm_nodes +=[nID]

                    at_fusion_graph.remove_nodes_from(rm_nodes)
                    pre_fusion_graph.remove_nodes_from(rm_nodes)

                    (agg_fusion_partners,) = at_fusion_graph.edges - pre_fusion_graph.edges
                    agg1_ID, agg2_ID = agg_fusion_partners
                    agg_fusion_partners = {agg1_ID, agg2_ID}
                    isolated_history = self._isolateMitoBallStructureHistory(post_case.history, agg_fusion_partners)
                    timestamps = [np.around(nData.get("timestamp")+post_case.time_interval, decimals=1) for _, nData in isolated_history.nodes(data = True)]
                    (timestamps, n_networks) = np.unique(timestamps, return_counts=True)
                    n_agg_over_time = [prior_aggCount if t<timestamp else nAgg for t in timestamps]
                    time_index = [-i+1 if i >=2 else 0 for i in range(len(n_agg_over_time))]
                    time_index.sort()
                    
                    for h,i,j,k in zip(time_index,timestamps, n_networks, n_agg_over_time):
                        output_dataframe.append([runID,*conditions,
                                                n_reduction_events,
                                                h,i,j,k])
                    n_reduction_events += 1
                prior_aggCount = nAgg
                prior_timestamp = timestamp
                
            output_dataframe = pd.DataFrame(output_dataframe, columns=["runID", *self.column_pre_header,
                                                                    "nth_agg_redction_event",
                                                                        "time_index","timestamp", 
                                                                        "n_networks", "n_agg_over_time"])
            return output_dataframe
        except:
            return None

    def getEventFrequency(self,
                          runID: str, post_pickle_path: str,
                          start_timestamp: float = 1200,
                          end_timestamp: float = 1500,
                          time_resolution: float = 10) -> pd.DataFrame:
        """Given a history directed graph, decrease the time resolution of the graph
        to multiples of time_resolution (in seconds) and sample from the range of time
        starting at start_timestamp and ending at end_timestamp.
        Determine the number of times each kind of event occurs during the time duration, 
        at the specified frame rate.

        Args:
            runID (str): simulation run ID prefix (used to ID simulation)
            post_pickle_path (str): path to serialized pickle object that records the simulation state
                post transition.  
            start_timestamp (float, optional): Timestamp to start sampling history at. Defaults to 1200 seconds.
            end_timestamp (float, optional): Timestamp to end sampling history at. Defaults to 1500 seconds.
            time_resolution (float, optional): time frame rate to drop simulation rate to. Defaults to 10 seconds.
                Note time_resolution must be lower than that of actual simulation time interval size (dt), cannot
                get greater resolution into simulation.

        Returns:
            pd.DataFrame: dataframe with columns:
                fileID <- fileID
                nAggs <- number of starting discrete aggregates
                DA <- diffusion constant parameter set
                FFI <- Fission/fusion parameter set
                targFis <- targeted fission on/off
                start_timestamp <- time starting at when history was examined
                end_timestamp <- end time for examining history
                time_resolution <- sampling time resolution to downsample existing data to.
        """
        # conditions = self.getExptConditions(runID)
        with open(post_pickle_path, 'rb') as f:
            post_case = pickle.load(f)
        # Keep nodes with timestamps within bounds set by start_timestamp and end_timestamp, and
        # that are multiples of the defined time resolution
        if post_case.time_interval == time_resolution:
            history_net_at_resolution = post_case.history
        elif time_resolution < post_case.time_interval:
            raise ValueError(
                "Cannot use time_resolution smaller than simulation time_interval")
        else:
            history_net_at_resolution = recreate_graph_at_time_resolution(post_case.history,
                                                                          resolution=time_resolution,
                                                                          start_time=start_timestamp,
                                                                          end_time=end_timestamp)
        # Uncomment for debugging plot
        # pos = nx.nx_agraph.graphviz_layout(history_net_at_resolution,
        #                         prog='dot',
        #                         args="-Grankdir=LR")
        # node_color = [history_net_at_resolution.nodes[node]['timestamp']
        #               for node in history_net_at_resolution.nodes()]
        # node_sizes = [len(history_net_at_resolution.nodes[node]['members'])
        #               for node in history_net_at_resolution.nodes()]
        # cmap = plt.cm.Reds
        # nx.draw_networkx(history_net_at_resolution,
        #                  pos=pos,
        #                  node_color=node_color,
        #                  node_size = node_sizes,
        #                  cmap=cmap,
        #                  with_labels=False,
        #                  connectionstyle="angle, angleA=-90,angleB=180, rad=0")
        # sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=min(node_color),
        #                                                          vmax=max(node_color)))
        # sm._A = []
        # cbar = plt.colorbar(sm)
        # cbar.set_label("timestamp")
        # plt.show()

        n_fusions, n_fissions, n_none, n_multiOps = 0, 0, 0, 0
        for nodeID in history_net_at_resolution.nodes():
            (_, postOp, *_) = self._getNodeOpHist(history_net_at_resolution,
                                                  query_nodeID=nodeID, time_precision=1)
            if postOp == "Fusion":
                n_fusions += 1
            elif postOp == "Fission":
                n_fissions += 1
            elif postOp == "None":
                n_none += 1
            else:
                n_multiOps += 1

        # Divide by two since fusions are double counted.
        n_fusions = n_fusions//2
        dataframe = [runID, #*conditions,
                     start_timestamp, end_timestamp, time_resolution,
                     ((end_timestamp - start_timestamp)/time_resolution)+1,
                     n_fusions, n_fissions, n_none, n_multiOps]
        dataframe = pd.DataFrame([dataframe], columns=["runID", #*self.column_pre_header,
                                                       "start_timestamp", "end_timestamp", "time_resolution",
                                                       "n_frames",
                                                       "n_fusions", "n_fissions", "n_none", "n_multiOps"])
        return dataframe

    def getInheritedEndAttr(self, runID: str, pre_pickle_path: str, post_pickle_path: str, limitLen: int) -> pd.DataFrame:
        """Given a number of nodes to pass to bud (limitLen), determine longest chain in
        network to be inherited and calculate attributes of that chain
        Runs inheritance on only the END state of the simulation

        Args:
            runID (str): simulation run ID prefix (used to ID simulation)
            pre_pickle_path (str): path to serialized pickle object that records the simulation state
                prior to transition.  
            post_pickle_path (str):  path to serialized pickle object that records the simulation state
                post transition. 
            limitLen (int): number of balls to pass in inheritance

        Returns:
            pd.DataFrame: dataframe with columns:
                fileID = fileID
                n_aggs_setting = number of starting aggregate balls
                diff_setting = diffusion coefficient settings
                ffi_setting = fission/fusion/idle settings
                targFis_setting = is targeted fission True(ON) or False(OFF)
                pre_n_bad_nodes = for pre case, number of bad agg balls inherited
                pre_n_discrete_aggs = for pre case, number of discrete aggs inherited
                pre_n_good_nodes = for pre case, number of good mito balls inherited
                post_n_bad_nodes = for post case, number of bad agg balls inherited
                post_n_discrete_aggs = for post case, number of discrete aggs inherited
                post_n_good_nodes = for post case, number of good mito balls inherited
        """
        print(runID)
        conditions = self.getExptConditions(runID)
        with open(pre_pickle_path, 'rb') as f:
            pre_case = pickle.load(f)

        with open(post_pickle_path, 'rb') as f:
            post_case = pickle.load(f)

        pre_inheritAttr = self._calcInheritedAttr(nx_network=pre_case.network,
                                                  limitLen=limitLen)
        post_inheritAttr = self._calcInheritedAttr(nx_network=post_case.network,
                                                   limitLen=limitLen)

        dataframe = [runID, *conditions, *pre_inheritAttr, *post_inheritAttr,
                     pre_case.count_nAggClusters(),
                     post_case.count_nAggClusters()]

        dataframe = pd.DataFrame([dataframe], columns=["runID", *self.column_pre_header,
                                                       "inherited_pre_n_bad_nodes", "inherited_pre_n_discrete_aggs", "inherited_pre_n_good_nodes",
                                                       "inherited_post_n_bad_nodes", "inherited_post_n_discrete_aggs", "inherited_post_n_good_nodes",
                                                       "pre_n_discrete_aggs", "post_n_discrete_aggs"])
        return dataframe

    def _calcInheritedAttr(self, nx_network, limitLen: int) -> Tuple[int, int, int]:
        """Determine the largest subgraph within the simulation network, and then determine the longest
        chain within the largest subgraph to pass for inheritance. Pass limitLen number of balls
        from the longest subchain for inheritance.

        Args:
            simulation_nx_state (nx.Graph): Simulation networkx graph state
            limitLen (int): number of balls to inherit

        Returns:
            Tuple[int, int, int]:
                n_bad_nodes <- number of bad nodes inherited
                n_discrete_aggs <- number of discrete aggs inherited
                n_good_nodes <- number of good nodes inherited
        """
        largest_subnet = getLargestSubnet(nx_network)
        # cast as nx.Graph to unfreeze
        largest_subnet = nx.Graph(largest_subnet)

        # # Comment for visualization of subgraph
        # # This part of position depends on original network that was used to generate
        # # subgraph since positions are indexed by node ID, not node index
        # pos = [p['pos'] for _, p in nx_network.nodes(data=True)]
        # # Color on the other hand is dependent on node index, not node ID
        # c = ["g" if not p['has_agg'] else "r" for _, p in largest_subnet.nodes(data=True)]
        # nx.draw(largest_subnet, pos=pos, node_color=c, with_labels=True)
        # plt.show()

        _, tipID = getLongestChain(largest_subnet)
        depth_dict = nx.single_source_dijkstra_path_length(largest_subnet,
                                                           tipID)
        inherited_nodeIDs = floodFillFromNode(
            depth_dict=depth_dict, max_filled=limitLen)
        n_bad_nodes = sum(
            [nx_network.nodes[pID]["obj"].has_agg for pID in inherited_nodeIDs])

        good_node_IDs = [
            pID for pID in inherited_nodeIDs if not nx_network.nodes[pID]["obj"].has_agg]
        n_good_nodes = len(inherited_nodeIDs) - n_bad_nodes

        # Determine n_discrete_aggs
        inherited_chain = largest_subnet.subgraph(list(inherited_nodeIDs))
        inherited_chain = nx.Graph(inherited_chain)
        inherited_chain.remove_nodes_from(good_node_IDs)
        n_discrete_aggs = len(list(nx.connected_components(inherited_chain)))

        return (n_bad_nodes, n_discrete_aggs, n_good_nodes)

    def getInheritanceOverTime(self, runID: str, pre_pickle_path: str, post_pickle_path: str,
                               limitLen: int, start_time: float = 600.0, time_resolution: float = 100.0) -> pd.DataFrame:
        """Given a simulation's state over time, get the evolution in probability of inheriting an
        aggregate over time starting at a specificed time, and at a specified time resolution.
        Since during recording of the simulation only remodeling changes to the simulation state are recorded,
        and not every specified time at time_resolution has a remodeling event occur, for each specified
        time resolution, the closest in time remodeling state prior to the specified time resolution is used
        as the simulation state at the sampling timestamp.

        NOTE: may throw error if inheritance algorithm is applied to simulation without aggregates (untested)

        Args:
            runID (str): simulation run ID prefix (used to ID simulation)
            pre_pickle_path (str): path to serialized pickle object that records the simulation state
                prior to transition.  This argument is not used for this function, but is only there
                as a placeholder for compatibility with its associated batch function, batch_getInheritanceOverTime.
            post_pickle_path (str):  path to serialized pickle object that records the simulation state
                post transition.  
            limitLen (int): number of balls to pass in inheritance
            start_time (float, optional): Timestamp to start sampling for inheritance. Typically set to
                first timestamp when aggregates are seeded. Units are in seconds. Defaults to 600.
            time_resolution (float, optional): Time resolution to sample at. Units are in seconds.
                Defaults to 100.

        Returns:
            pd.DataFrame: Dataframe with columns:
                fileID <- fileID
                n_aggs_setting <- number of starting aggregate balls
                diff_setting <- diffusion coefficient settings
                ffi_setting <- fission/fusion/idle settings
                targFis_setting <- is targeted fission True(ON) or False(OFF)
                last_timestep <- actual timestamp in simulation where last network state is used for inheritance
                    calculation
                sample_timestep <- selected timestamp (modulo of time_resolution)
                n_bad_nodes <- number of bad nodes inherited
                n_discrete_aggs <- number of discrete aggs inherited
                n_good_nodes <- number of good nodes inherited
        """
        dataframe = []

        conditions = self.getExptConditions(runID)
        with open(post_pickle_path, 'rb') as f:
            post_case = pickle.load(f)
        all_nx_timestamps = list(post_case.network_history.keys())
        all_nx_timestamps.sort()
        max_ts = np.around(post_case.timestamp, decimals=1)

        sampling_timestamps = np.arange(
            start_time, max_ts+1, time_resolution)
        selected_timestamps = []

        # Because during recording of simulation, there are floating point errors
        # (>1e-7) during record keeping of timestamp, identify relevant time stamps that are
        # close to the sampling timestamps
        # For the last time stamp, grab the final timestmap in the simulation
        idx = 0
        for sample_ts in sampling_timestamps:
            prior_actual_ts = None
            for actual_ts in all_nx_timestamps[idx:]:
                idx += 1
                if actual_ts == sample_ts:
                    selected_timestamps += [actual_ts]
                    break
                if actual_ts <= sample_ts:
                    if idx == len(all_nx_timestamps) - 1:
                        selected_timestamps += [actual_ts]
                        break
                    else:
                        prior_actual_ts = actual_ts
                if actual_ts > sample_ts:
                    selected_timestamps += [actual_ts]
                    break

        # For each of sampled timestamps, get network at the time and run inheritance algorithm on it
        for ts_key, sampling_ts in zip(selected_timestamps, sampling_timestamps):
            network_at_ts = post_case.network_history[ts_key]

            n_bad_nodes, n_discrete_aggs, n_good_nodes = self._calcInheritedAttr(network_at_ts,
                                                                                 limitLen=limitLen)

            # pos = [p['pos'] for _, p in network_at_ts.nodes(data=True)]
            # c = ["g" if not p['has_agg'] else "r" for _, p in network_at_ts.nodes(data=True)]
            # nx.draw(network_at_ts, pos=pos, node_color=c, with_labels=True)
            # plt.show()
            # raise
            dataframe.append([runID, *conditions, ts_key, sampling_ts,
                              n_bad_nodes, n_discrete_aggs, n_good_nodes])

        dataframe = pd.DataFrame(dataframe, columns=["runID", *self.column_pre_header,
                                                     "last_timestep", "sample_timestep",
                                                     "n_bad_nodes", "n_discrete_aggs", "n_good_nodes"])
        return dataframe

    def getEndNAggClusters(self, runID: str, pre_pickle_path: str, post_pickle_path: str) -> pd.DataFrame:
        """Get number of aggregate clusters at beginning and end of each pair of simulation states

        Args:
            runID (str): simulation run ID prefix (used to ID simulation)
            pre_pickle_path (str): path to serialized pickle object that records the simulation state
                prior to transition.
            post_pickle_path (str):  path to serialized pickle object that records the simulation state
                post transition.  

        Returns:
            pd.DataFrame: dataframe with columns:
                fileID = fileID
                n_aggs_setting = number of starting aggregate balls
                diff_setting = diffusion coefficient settings
                ffi_setting = fission/fusion/idle settings
                targFis_setting = is targeted fission True(ON) or False(OFF)
                pre_nAggs = number of discrete aggs at end of pre state
                post_nAggs = number of discrete aggs at end of post state
        """

        conditions = self.getExptConditions(runID)
        with open(pre_pickle_path, 'rb') as f:
            pre_case = pickle.load(f)

        with open(post_pickle_path, 'rb') as f:
            post_case = pickle.load(f)

        # condition_name = self.getConditionName(pickle_path)
        dataframe = [runID, *conditions,
                     pre_case.count_nAggClusters(),
                     post_case.count_nAggClusters()]
        dataframe = pd.DataFrame([dataframe], columns=["runID",
                                                       *self.column_pre_header,
                                                       "pre_nAggs", "post_nAggs"])
        return dataframe

    def getDeltaAggInterSep(self, runID: str, pre_pickle_path: str, post_pickle_path: str) -> pd.DataFrame:
        """Traverses complete experiment filelist to get pre and post
        inter-aggregate ball separation and number of discrete aggregates

        Args:
            runID (str) : fileID from self.unique_runIDs
            pre_pickle_path (str) : path to pickle file with saved state of simulation at transition point
            post_pickle_path (str) : path to pickle file with whole simulation

        Calls:
            self.calcDeltaAggInterSep

        Returns:
            pd.DataFrame: Dataframe with single row and columns:
                fileID = fileID
                n_aggs_setting = number of starting aggregate balls
                diff_setting = diffusion coefficient settings
                ffi_setting = fission/fusion/idle settings
                targFis_setting = is targeted fission True(ON) or False(OFF)
                pre_agg_avgInterSep = at end of WT equilibriation, avg agg ball interseparation
                pre_nAgg = at end of WT equilibriation, n discrete aggregates, should be same as n_aggs_setting
                pre_agg_avgInterSep = at end of experiment, avg agg ball interseparation
                post_nAgg = at end of experiment, n discrete aggregates
        """
        conditions = self.getExptConditions(runID)
        with open(pre_pickle_path, 'rb') as f:
            pre_case = pickle.load(f)

        with open(post_pickle_path, 'rb') as f:
            post_case = pickle.load(f)
        pre_agg_attr = self._calcDeltaAggInterSep(pre_case)
        post_agg_attr = self._calcDeltaAggInterSep(post_case)
        dataframe = [runID, *conditions, *pre_agg_attr, *post_agg_attr]
        dataframe = pd.DataFrame([dataframe], columns=["runID", *self.column_pre_header,
                                                       "pre_agg_avgInterSep", "pre_nAgg",
                                                       "post_agg_avgInterSep", "post_nAgg"])
        return dataframe

    def _calcDeltaAggInterSep(self, simulation_pickle: Simulation) -> Tuple[float, int]:
        """Given a simulation object, determine the number of discrete aggregates displayed
        in the simulation and the average inter-ball separation between individual aggregate balls.
        For instance, given that there are 6 aggregate balls in the simulation, they can undergo
        fission and fusion, so by the end of the simulation, they may have formed a cohesive ball.
        The number of discrete aggregates corresponds to the number of clusters formed by the 6 balls,
        the average inter-ball separation refers to the average distance between the centers of each ball

        Args:
            simulation_pickle (Simulation): Simulation pickle object saved by driver.py

        Returns:
            Tuple[float, int]: (average inter-agg ball separation, n discrete aggs)
        """
        # WARNING DO NOT USE NETWORKX AGGREGATE STATUS AS ACCURATE,
        # IT IS NOT UPDATED WITH SIMULATION RUN DURING TRANSITION TO
        # AGGREGATE STATE

        agg_pos_list = []
        rm_nodes = []
        for n, ndata in simulation_pickle.network.nodes(data=True):
            if ndata['obj'].has_agg:
                agg_pos_list.append(ndata['obj'].position)
            else:
                rm_nodes += [n]
        simulation_pickle.network.remove_nodes_from(rm_nodes)
        n_discrete_agg = [1 for _ in nx.connected_components(
            simulation_pickle.network)]
        n_discrete_agg = len(n_discrete_agg)
        agg_ball_intersep = pdist(agg_pos_list)
        agg_ball_intersep_avg = np.mean(agg_ball_intersep)
        return (agg_ball_intersep_avg, n_discrete_agg)

    def getFirstAggFusionEventTime(self, runID: str, pre_pickle_path: str, post_pickle_path: str) -> pd.DataFrame:
        """Get the first timestamp where the number of discrete aggregates
        decreases from the previous timestamp.  The time it takes for the aggregates to
        consolidate can be calculated from this value by subtracting the time at which the aggregate
        bodies were seeded into the simulation
        Args:
            runID (str): simulation run ID prefix (used to ID simulation)
            pre_pickle_path (str): path to serialized pickle object that records the simulation state
                prior to transition.  This argument is not used for this function, but is only there
                as a placeholder for compatibility with its associated batch function, batch_getFirstAggFusionEventTime.
            post_pickle_path (str):  path to serialized pickle object that records the simulation state
                post transition.  
        Returns:
            pd.DataFrame: pandas dataframe with columns:
                fileID <- fileID
                nAggs <- number of starting discrete aggregates
                DA <- diffusion constant parameter set
                FFI <- Fission/fusion parameter set
                targFis <- targeted fission on/off
                f_timestamp <- first timestamp where a decrease in aggregate number is observed.

        """

        conditions = self.getExptConditions(runID)
        with open(post_pickle_path, 'rb') as f:
            post_case = pickle.load(f)
        n_agg_hist = post_case.nAgg_history
        timestamps = np.arange(len(n_agg_hist)) * post_case.time_interval
        recorded_timestamps = list(post_case.network_history.keys())
        new_ts = time_alignment(timestamps, recorded_timestamps)

        time_of_reduction = -1
        for i, (pre, post) in enumerate(zip(n_agg_hist[:-1], n_agg_hist[1:])):
            if post < pre:
                time_of_reduction = new_ts[i]
                break
        dataframe = [runID, *conditions, time_of_reduction]
        dataframe = pd.DataFrame([dataframe], columns=["runID",
                                                       *self.column_pre_header,
                                                       "time_of_reduction"])
        return dataframe

    def getNSubnetsOverTime(self, runID: str, pre_pickle_path: str, post_pickle_path: str, time_resolution: float = 100.0) -> pd.DataFrame:
        """Get the number of discrete mitochondria networks over time at a
        defined time resolution

        Args:
            runID (str): simulation run ID prefix (used to ID simulation)
            pre_pickle_path (str): path to serialized pickle object that records the simulation state
                prior to transition.  This argument is not used for this function, but is only there
                as a placeholder for compatibility with its associated batch function, batch_getNSubnetsOverTime.
            post_pickle_path (str):  path to serialized pickle object that records the simulation state
                post transition.  
            time_resolution (float, optional): time resolution in seconds. Defaults to 100.0 seconds.

        Returns:
            pd.DataFrame: dataframe with columns:
                fileID = fileID
                n_aggs_setting = number of starting aggregate balls
                diff_setting = diffusion coefficient settings
                ffi_setting = fission/fusion/idle settings
                targFis_setting = is targeted fission True(ON) or False(OFF)
                timestamp = timestamp
                n_discrete_mito = n_discrete_mito at timestamp
        """
        dataframe = []
        conditions = self.getExptConditions(runID)
        with open(post_pickle_path, 'rb') as f:
            post_case = pickle.load(f)

        nodesatResolution = [(x, np.around(y['timestamp'], 1)) for x, y in post_case.history.nodes(
            data=True) if np.around(y['timestamp'], 1) % time_resolution == 0.0]
        nnodesattime = {k[-1]: 0 for k in nodesatResolution}
        for _, timestamp in nodesatResolution:
            nnodesattime[timestamp] += 1
        for tstamp, nObj in nnodesattime.items():
            dataframe.append([runID, *conditions,
                              tstamp, nObj])
        dataframe = pd.DataFrame(dataframe, columns=["runID",
                                                     *self.column_pre_header,
                                                     "timestamp", "n_discrete_mito"])
        return dataframe

    def getVolumesOverTime(self, runID: str, pre_pickle_path: str, post_pickle_path: str, time_resolution: float = 100.0) -> pd.DataFrame:
        """Get the individual volumes of each discrete subnetwork
        in the simulation state at multiple timestamps at a specified time_resolution

        Args:
            runID (str): simulation run ID prefix (used to ID simulation)
            pre_pickle_path (str): path to serialized pickle object that records the simulation state
                prior to transition.  This argument is not used for this function, but is only there
                as a placeholder for compatibility with its associated batch function, batch_getVolumesOverTime.
            post_pickle_path (str):  path to serialized pickle object that records the simulation state
                post transition.  
            time_resolution (float): timestamp in seconds. Defaults to 100.0 seconds.

        Returns:
            pd.DataFrame: dataframe with columns:
                fileID = fileID
                n_aggs_setting = number of starting aggregate balls
                diff_setting = diffusion coefficient settings
                ffi_setting = fission/fusion/idle settings
                targFis_setting = is targeted fission True(ON) or False(OFF)
                timestamp = timestamp
                sim_mito_vol = mitochondria volume in simulation
        """
        dataframe = []
        conditions = self.getExptConditions(runID)
        with open(post_pickle_path, 'rb') as f:
            post_case = pickle.load(f)

        nodesatTimestamp = [(len(y['members']), np.around(y['timestamp'], 1)) for x, y in post_case.history.nodes(
            data=True) if np.around(y['timestamp'], 1) % time_resolution == 0.0]
        for nMembers, timestamp in nodesatTimestamp:
            dataframe.append([runID, *conditions,
                              timestamp, nMembers*PARTICLE_VOLUME])

        dataframe = pd.DataFrame(dataframe, columns=["runID",
                                                     *self.column_pre_header,
                                                     "timestamp", "sim_mito_vol"])
        return dataframe

    def getVolumesAtTime(self, runID: str, pre_pickle_path: str, post_pickle_path: str, timestamp: float) -> pd.DataFrame:
        """Get the individual volumes of each discrete subnetwork
        in the simulation state at one specific timestamp

        Args:
            runID (str): simulation run ID prefix (used to ID simulation)
            pre_pickle_path (str): path to serialized pickle object that records the simulation state
                prior to transition.  This argument is not used for this function, but is only there
                as a placeholder for compatibility with its associated batch function, batch_getVolumesOverTime.
            post_pickle_path (str):  path to serialized pickle object that records the simulation state
                post transition.  
            timestamp (float): timestamp in seconds. 

        Returns:
            pd.DataFrame: dataframe with columns:
                fileID = fileID
                n_aggs_setting = number of starting aggregate balls
                diff_setting = diffusion coefficient settings
                ffi_setting = fission/fusion/idle settings
                targFis_setting = is targeted fission True(ON) or False(OFF)
                timestamp = timestamp
                sim_mito_vol = mitochondria volume in simulation
        """
        dataframe = []
        # conditions = self.getExptConditions(runID)
        with open(post_pickle_path, 'rb') as f:
            post_case = pickle.load(f)

        nodesatTimestamp = [(len(y['members']), np.around(y['timestamp'], 1)) for x, y in post_case.history.nodes(
            data=True) if np.around(y['timestamp'], 1) == timestamp]

        for nMembers, timestamp in nodesatTimestamp:
            dataframe.append([runID, #*conditions,
                              timestamp, nMembers*PARTICLE_VOLUME])

        dataframe = pd.DataFrame(dataframe, columns=["runID",
                                                     #*self.column_pre_header,
                                                     "timestamp", "sim_mito_vol"])
        return dataframe

    def getNAggsOverTime(self, runID: str, pre_pickle_path: str, post_pickle_path: str, time_resolution: float = 100.0) -> pd.DataFrame:
        """Extracts the number of disrete aggregates over the course of the simulation.
        Utilizes the final state of the simulation (beyond the equilibriation measurement at
        the time when the simulation parameters are altered) to gather the number of aggregates
        from the beginning of the simulation until the end.

        NOTE: Because this function utilizes the full simulation data, there will be an initial
        window where no aggregates are recorded since there are no aggregates seeded at the
        beginning of the simulation.

        Args:
            runID (str): simulation run ID prefix (used to ID simulation)
            pre_pickle_path (str): path to serialized pickle object that records the simulation state
                prior to transition.  This argument is not used for this function, but is only there
                as a placeholder for compatibility with its associated batch function, batch_getNAggsOverTime.
            post_pickle_path (str):  path to serialized pickle object that records the simulation state
                post transition.  
            time_resolution (float, optional): Time resolution to sample at. Units are in centiseconds.
                Defaults to 100.

        Returns:
            pd.DataFrame: Pandas dataframe with columns:
                fileID <- fileID
                nAggs <- number of starting discrete aggregates
                DA <- diffusion constant parameter set
                FFI <- Fission/fusion parameter set
                targFis <- targeted fission on/off
                timestamp <- timestamp in centi seconds
                n_aggs <- number of aggregates at each timestamp

        """
        conditions = self.getExptConditions(runID)
        with open(post_pickle_path, 'rb') as f:
            post_case = pickle.load(f)
        time_interval = int(time_resolution/post_case.time_interval)
        n_agg_hist = post_case.nAgg_history[::time_interval]
        timestamps = np.arange(len(n_agg_hist)) * time_interval
        dataframe = {"timestamp": timestamps,
                     "n_aggs_at_time": n_agg_hist}
        dataframe = pd.DataFrame(dataframe, index=None)
        dataframe['fileID'] = runID
        dataframe['nStartingAggs'] = conditions[-4]
        dataframe['DA'] = conditions[-3]
        dataframe['FFI'] = conditions[-2]
        dataframe['targFis'] = conditions[-1]
        return dataframe

    def batch_nonTimecourseAnalyses(self, func: Callable) -> pd.DataFrame:
        """Helper function for class functions that query simulation at a single point.
        Given an unwrapped function in the Experiment class, run the given function over the whole
        experiment in batch.

        Args:
            func (Callable): Function to be used to query experiment files.

        Returns:
            pd.DataFrame:  Depends on func input. See provided func documentation for more details.
        """
        args_to_feed = []
        for runID, run_simPathDict in self.unique_runIDs.items():
            args_to_feed += [[self,
                              runID,
                              run_simPathDict['pre'],
                              run_simPathDict['post']]]

        with multiprocessing.Pool() as p:
            result = p.map(func, args_to_feed)

        merged_df = pd.concat(result)
        return merged_df

    def batch_timecourseAnalyses(self, func: Callable, time_resolution: float = 100.0) -> pd.DataFrame:
        """Helper function for class functions that query simulation at multiple time points.
        Given an unwrapped function in the Experiment class, run the given function over the whole
        experiment in batch with the provided time_resolution to sample at.

        Args:
            func (Callable): Function to be used to query experiment
            time_resolution (float, optional): Time resolution to sample at. Defaults to 100.0 seconds.

        Returns:
            pd.DataFrame: Depends on func input. See provided func documentation for more details.
        """
        args_to_feed = []
        for runID, run_simPathDict in self.unique_runIDs.items():
            args_to_feed += [[self,
                              runID,
                              run_simPathDict['pre'],
                              run_simPathDict['post'],
                              time_resolution]]
        with multiprocessing.Pool() as p:
            result = p.map(func, args_to_feed)
        merged_df = pd.concat(result)
        return merged_df

    def batch_analysisAtTimestamp(self, func: Callable, timestamp: float) -> pd.DataFrame:
        """Helper function for class functions that query simulation at a single time point.
        Given an unwrapped function in the Experiment class, run the given function over the whole
        experiment in batch with the provided time_resolution to sample at.

        Args:
            func (Callable): Function to be used to query experiment
            timestamp (float, optional): Time resolution to sample at. Defaults to 100.0 seconds.

        Returns:
            pd.DataFrame: Depends on func input. See provided func documentation for more details.
        """
        args_to_feed = []
        for runID, run_simPathDict in self.unique_runIDs.items():
            args_to_feed += [[self,
                              runID,
                              run_simPathDict['pre'],
                              run_simPathDict['post'],
                              timestamp]]
        with multiprocessing.Pool() as p:
            result = p.map(func, args_to_feed)
        merged_df = pd.concat(result)
        return merged_df
    
    def batch_getAggReductionMitoEvents(self) -> pd.DataFrame:
        """See self.getAggReductionMitoEvents for more documentation
        This function is the batch equivalent of self.getEndNAggClusters.

        Returns:
            pd.DataFrame: see self.getEndNAggClusters for more info
        """
        return self.batch_nonTimecourseAnalyses(func=unwrapped_experiment_getAggReductionMitoEvents)

    def batch_getEventFrequency(self, start_timestamp: float, end_timestamp: float, time_resolution: float = 10) -> pd.DataFrame:
        args_to_feed = []
        for runID, run_simPathDict in self.unique_runIDs.items():
            args_to_feed += [[self,
                              runID,
                              run_simPathDict['post'],
                              start_timestamp,
                              end_timestamp,
                              time_resolution]]
        with multiprocessing.Pool() as p:
            result = p.map(
                unwrapped_experiment_getEventFrequency, args_to_feed)
        merged_df = pd.concat(result)
        return merged_df

        raise NotImplementedError

    def batch_getInheritedEndAttr(self, limitLen: int = CHAIN_LENGTH) -> pd.DataFrame:
        """see self.getInheritedEndAttr for more documentation. 
        This function is the batch equivalent of self.getInheritedEndAttr.

        Args:
            limitLen (int, optional): Chain length to query inheritance with. Defaults to CHAIN_LENGTH.

        Returns:
            pd.DataFrame: see self.getInheritedEndAttr for more documentation
        """
        args_to_feed = []
        for runID, run_simPathDict in self.unique_runIDs.items():
            args_to_feed += [[self,
                              runID,
                              run_simPathDict['pre'],
                              run_simPathDict['post'],
                              limitLen]]

        with multiprocessing.Pool() as p:
            result = p.map(
                unwrapped_experiment_getInheritedEndAttr, args_to_feed)
        merged_df = pd.concat(result)
        return merged_df

    def batch_getInheritanceOverTime(self, limitLen: int = CHAIN_LENGTH,
                                     start_time: float = 600, time_resolution: float = 100.0) -> pd.DataFrame:
        """Get inheritance statistics over time; measures what inheritance would look like
        if it were to occur at any given time point in the simulation.

        This function is identical to self.getInheritanceOverTime, but is built for running in batch over 
        a large number of simulations.

        Args:
            limitLen (int, optional): Number of balls to pass down in inheritance. Defaults to CHAIN_LENGTH.
            start_time (float, optional): time to start querying inheritance. Defaults to 600 seconds.
            time_resolution (float, optional): time resolution to query inheritance at. Defaults to 100.0 seconds.

        Returns:
            pd.DataFrame: Dataframe with columns identical to that of self.getInheritanceOverTime;
                see comment there for more info.

        """
        args_to_feed = []
        for runID, run_simPathDict in self.unique_runIDs.items():
            args_to_feed += [[self,
                              runID,
                              run_simPathDict['pre'],
                              run_simPathDict['post'],
                              limitLen, start_time, time_resolution]]

        with multiprocessing.Pool() as p:
            result = p.map(unwrapped_experiment_getInheritanceOverTime,
                           args_to_feed)
        merged_df = pd.concat(result)
        return merged_df

    def batch_getEndNAggClusters(self) -> pd.DataFrame:
        """See self.getEndNAggClusters for more documentation
        This function is the batch equivalent of self.getEndNAggClusters.

        Returns:
            pd.DataFrame: see self.getEndNAggClusters for more info
        """
        return self.batch_nonTimecourseAnalyses(func=unwrapped_experiment_getEndNAggClusters)

    def batch_getDeltaAggInterSep(self) -> pd.DataFrame:
        """See self.getDeltaAggInterSep for more documentation
        This function is the batch equivalent of self.getDeltaAggInterSep.

        Returns:
            pd.DataFrame: see self.getDeltaAggInterSep for more info
        """
        return self.batch_nonTimecourseAnalyses(func=unwrapped_experiment_getDeltaAggInterSep)

    def batch_getFirstAggFusionEventTime(self) -> pd.DataFrame:
        """See self.getFirstAggFusionEventTime for more documentation
        This function is the batch equivalent of self.getFirstAggFusionEventTime.

        Returns:
            pd.DataFrame: see self.getFirstAggFusionEventTime for more info
        """
        return self.batch_nonTimecourseAnalyses(func=unwrapped_experiment_getFirstAggFusionEventTime)

    def batch_getNSubnetsOverTime(self, time_resolution: float = 100.0) -> pd.DataFrame:
        """See self.getNSubnetsOverTime for more documentation
        This function is the batch equivalent of self.getNSubnetsOverTime.

        Args:
            time_resolution (float, optional): time resolution interval to sample at. Defaults to 100.0 seconds.

        Returns:
            pd.DataFrame: see self.getNSubnetsOverTime for column info
        """
        return self.batch_timecourseAnalyses(func=unwrapped_experiment_getNSubnetsOverTime, time_resolution=time_resolution)

    def batch_getVolumesOverTime(self, time_resolution: float = 100.0) -> pd.DataFrame:
        """See self.getVolumesOverTime for more documentation
        This function is the batch equivalent of self.getVolumesOverTime.

        Args:
            time_resolution (float, optional): time resolution interval to sample at. Defaults to 100.0 seconds.

        Returns:
            pd.DataFrame: see self.getVolumesOverTime for column info
        """
        return self.batch_timecourseAnalyses(func=unwrapped_experiment_getVolumesOverTime, time_resolution=time_resolution)

    def batch_getVolumesAtTime(self, timestamp: float) -> pd.DataFrame:
        """see self.getVolumesAtTime for more documentation. 
        This function is the batch equivalent of self.getVolumesAtTime.

        Args:
            timestamp (float): Timestamp to sample volumes at

        Returns:
            pd.DataFrame: see self.getVolumesAtTime for more documentation
        """
        return self.batch_analysisAtTimestamp(func=unwrapped_experiment_getVolumesAtTime, timestamp=timestamp)

    def batch_getNAggsOverTime(self, time_resolution: float = 100.0) -> pd.DataFrame:
        """See self.getNAggsOverTime for more documentation
        This function is the batch equivalent of self.getNAggsOverTime.

        Args:
            time_resolution (float, optional): time resolution interval to sample at. Defaults to 100.0 seconds.

        Returns:
            pd.DataFrame: see self.getNAggsOverTime for column info
        """
        return self.batch_timecourseAnalyses(func=unwrapped_experiment_getNAggsOverTime, time_resolution=time_resolution)


